{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294763a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# 加载MAT文件\n",
    "data_m = loadmat(\".\\\\6train_data_withCorrelation_20.mat\")\n",
    "train_data = data_m[\"train_data\"]  # 2998*10的矩阵\n",
    "\n",
    "# 提取数据\n",
    "X1 = []\n",
    "X2 = []\n",
    "Y = []\n",
    "for i in range(1440):\n",
    "    d = train_data[0][i]\n",
    "    d = np.transpose(d)\n",
    "    x1 = d[2:][:][::-1]\n",
    "    x2 = d[0:2][::-1]\n",
    "    y = d[0][-1]\n",
    "    X1.append(x1)\n",
    "    X2.append(x2)\n",
    "    Y.append(y)\n",
    "data_x1 = np.array(X1)\n",
    "data_x2 = np.array(X2)\n",
    "data_y = np.expand_dims(np.array(Y), axis=1)\n",
    "\n",
    "# 分配训练集和测试集\n",
    "tscv = TimeSeriesSplit(n_splits=4)\n",
    "list_train = []\n",
    "list_test = []\n",
    "for train, test in tscv.split(data_x1):\n",
    "    list_train.append(train)\n",
    "    list_test.append(test)\n",
    "num2 = list_train[3]\n",
    "num1 = list_test[3]\n",
    "\n",
    "# 划分训练集和测试集\n",
    "x1_train = data_x1[num2]\n",
    "x2_train = data_x2[num2]\n",
    "y_train = data_y[num2]\n",
    "print('x1_train', np.shape(x1_train))\n",
    "print('x2_train', np.shape(x2_train))\n",
    "print('y_train', np.shape(y_train))\n",
    "x1_test = data_x1[num1]\n",
    "x2_test = data_x2[num1]\n",
    "y_test = data_y[num1]\n",
    "print('x1_test', np.shape(x1_test))\n",
    "print('x2_test', np.shape(x2_test))\n",
    "print('y_test', np.shape(y_test))\n",
    "\n",
    "# 生成批次\n",
    "train_queue = tf.train.slice_input_producer([x1_train, x2_train, y_train], shuffle=None)\n",
    "val_queue = tf.train.slice_input_producer([x1_test, x2_test, y_test], shuffle=None)\n",
    "\n",
    "batch_xt1, batch_xt2, batch_yt = tf.train.batch(train_queue, batch_size=1152, capacity=1440)\n",
    "batch_xv1, batch_xv2, batch_yv = tf.train.batch(val_queue, batch_size=288, capacity=1440)\n",
    "print('batch_xv1', np.shape(batch_xv1))\n",
    "print('batch_xv2', np.shape(batch_xv2))\n",
    "print('batch_yv', np.shape(batch_yv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c2510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # TensorFlow>2.2\n",
    "import numpy as np\n",
    "\n",
    "# Encoder\n",
    "enc_inp1 = tf.keras.Input(shape=(5, 55), name=\"inp1\")  # 过去时间步的输入数据\n",
    "enc_inp2 = tf.keras.Input(shape=(2, 55), name=\"inp2\")  # 未来时间步的输入数据\n",
    "print('enc_inp1', enc_inp1.shape)\n",
    "print('enc_inp2', enc_inp2.shape)\n",
    "print('enc_inp2[:,0,:] is the recent data, enc_inp2[:,1,:] is the future data')\n",
    "\n",
    "# Decoder\n",
    "expected_output = tf.keras.Input(shape=(1,), name=\"expected_output\")  # 预期输出\n",
    "pharse = tf.keras.Input(shape=(), dtype=tf.bool, name='training')  # 是否处于训练阶段的标志位\n",
    "\n",
    "layers_stacked_count = 2\n",
    "hidden_dim = 64\n",
    "de_length = 2\n",
    "output_dim = 6\n",
    "alpha = 0.1  # 输出损失1的权重\n",
    "alpha_w = 0.01  # L2 正则化项的权重\n",
    "\n",
    "# Encoder cell\n",
    "cells = []\n",
    "for i in range(layers_stacked_count):\n",
    "    cells.append(tf.keras.layers.GRUCell(hidden_dim))\n",
    "cell = tf.keras.layers.StackedRNNCells(cells)\n",
    "\n",
    "enc_outputs, enc_memory = tf.keras.layers.RNN(cell, return_sequences=True)(enc_inp1)# 编码器的输出和状态\n",
    "\n",
    "print('enc_outputs', enc_outputs.shape)\n",
    "\n",
    "# Decoder cell\n",
    "decells = []\n",
    "for i in range(layers_stacked_count):\n",
    "    decells.append(tf.keras.layers.GRUCell(hidden_dim))\n",
    "decell = tf.keras.layers.StackedRNNCells(decells)\n",
    "\n",
    "dec_state = enc_memory  # 解码器初始状态为编码器的状态\n",
    "dec_inp = enc_inp2[:, 1, -6:]  # 解码器初始输入为未来时间步的数据\n",
    "dec_outputs = []\n",
    "outs = []\n",
    "\n",
    "w_h = tf.Variable(tf.random.normal([hidden_dim, 6]))  # 权重矩阵\n",
    "b_h = tf.Variable(tf.random.normal([6]))  # 偏置向量\n",
    "\n",
    "for time_step in range(de_length):\n",
    "    if time_step > 0:\n",
    "        dec_inp = out1  # 在非第一个时间步时，解码器的输入为上一个时间步的输出\n",
    "    dec_output, dec_state = decell(dec_inp, dec_state)  # 解码器的输出和状态\n",
    "    out1 = tf.matmul(dec_output, w_h) + b_h  # 解码器的输出\n",
    "    if pharse:\n",
    "        dec_inp = enc_inp2[:, time_step + 1, -6:]  # 在训练阶段，下一个时间步的输入为真实未来数据\n",
    "    else:\n",
    "        dec_inp = out1  # 在测试阶段，下一个时间步的输入为上一个时间步的输出\n",
    "    outs.append(out1)\n",
    "    dec_outputs.append(dec_output)\n",
    "\n",
    "print('dec_state', dec_state.shape)\n",
    "\n",
    "w_out = tf.Variable(tf.random.normal([hidden_dim, output_dim]))  # 输出层权重矩阵\n",
    "b_out = tf.Variable(tf.random.normal([output_dim]))  # 输出层偏置向量\n",
    "\n",
    "pre_output1 = outs[0]  # 预测的输出1\n",
    "pre_output2 = tf.matmul(dec_outputs[-1], w_out) + b_out  # 预测的输出2\n",
    "\n",
    "print(pre_output1.shape)\n",
    "print(pre_output2.shape)\n",
    "\n",
    "# Loss\n",
    "output_loss1 = tf.sqrt(tf.reduce_mean(tf.square(enc_inp2[:, 1, -7:-1] - pre_output1)))  # 输出损失1：目标点之前到现在的点的均方根误差\n",
    "output_loss2 = tf.sqrt(tf.reduce_mean(tf.square(expected_output - pre_output2)))  # 输出损失2：预测输出与目标输出的均方根误差\n",
    "\n",
    "reg_loss = tf.reduce_sum([tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    "                          if not (\"Bias\" in tf_var.name or \"Output_\" in tf_var.name)])  # L2 正则化项\n",
    "\n",
    "loss = alpha * output_loss1 + output_loss2 + alpha_w * reg_loss  # 总损失函数\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.999)\n",
    "train_op = optimizer.minimize(loss)\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "# Evaluation metrics\n",
    "acc = 1 - tf.reduce_mean(tf.sqrt(tf.square(expected_output - pre_output2)) / tf.sqrt(tf.square(expected_output)))\n",
    "tf.summary.scalar('ACC', acc)\n",
    "\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(pre_output2 - expected_output)))\n",
    "tf.summary.scalar('RMSE', rmse)\n",
    "\n",
    "unexplained_error = tf.reduce_sum(tf.square(expected_output - pre_output2))\n",
    "total_error = tf.reduce_sum(tf.square(expected_output - tf.reduce_mean(expected_output, axis=0)))\n",
    "r_square = 1. - tf.divide(unexplained_error, total_error)\n",
    "tf.summary.scalar('R_square', r_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9623f09d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
